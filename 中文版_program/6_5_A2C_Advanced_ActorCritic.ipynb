{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.5 以PyTorch建置A2C（Advanced Actor-Critic）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import套件\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import gym\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 設定常數\n",
    "ENV = 'CartPole-v0'  # 使用的課題名稱\n",
    "GAMMA = 0.99  # 時間折扣率\n",
    "MAX_STEPS = 200  # 1回合的step數\n",
    "NUM_EPISODES = 1000  # 最大回合數\n",
    "\n",
    "NUM_PROCESSES = 32  # 同時執行的環境\n",
    "NUM_ADVANCED_STEP = 5  # 設定前進幾步之後再計算報酬總和\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 設定計算A2C損失函數所需的常數\n",
    "value_loss_coef = 0.5\n",
    "entropy_coef = 0.01\n",
    "max_grad_norm = 0.5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 記憶體類別的定義\n",
    "\n",
    "\n",
    "class RolloutStorage(object):\n",
    "    '''為了執行Advantage學習建置的記憶體類別'''\n",
    "\n",
    "    def __init__(self, num_steps, num_processes, obs_shape):\n",
    "\n",
    "        self.observations = torch.zeros(num_steps + 1, num_processes, 4)\n",
    "        self.masks = torch.ones(num_steps + 1, num_processes, 1)\n",
    "        self.rewards = torch.zeros(num_steps, num_processes, 1)\n",
    "        self.actions = torch.zeros(num_steps, num_processes, 1).long()\n",
    "\n",
    "        # 儲存折扣報酬總和\n",
    "        self.returns = torch.zeros(num_steps + 1, num_processes, 1)\n",
    "        self.index = 0  # insert索引\n",
    "\n",
    "    def insert(self, current_obs, action, reward, mask):\n",
    "        '''將transition存入下個index'''\n",
    "        self.observations[self.index + 1].copy_(current_obs)\n",
    "        self.masks[self.index + 1].copy_(mask)\n",
    "        self.rewards[self.index].copy_(reward)\n",
    "        self.actions[self.index].copy_(action)\n",
    "\n",
    "        self.index = (self.index + 1) % NUM_ADVANCED_STEP  # 更新索引\n",
    "\n",
    "    def after_update(self):\n",
    "        '''執行Advantage的step數結束後，將最新的結果存入index0'''\n",
    "        self.observations[0].copy_(self.observations[-1])\n",
    "        self.masks[0].copy_(self.masks[-1])\n",
    "\n",
    "    def compute_returns(self, next_value):\n",
    "        '''加總於各步驟執行Advantage學習所得的折扣報酬'''\n",
    "\n",
    "        # 注意：從第5step開始反向計算\n",
    "        # 注意：第5step為Advantage1。第4step為Advantage2。・・・\n",
    "        self.returns[-1] = next_value\n",
    "        for ad_step in reversed(range(self.rewards.size(0))):\n",
    "            self.returns[ad_step] = self.returns[ad_step + 1] * \\\n",
    "                GAMMA * self.masks[ad_step + 1] + self.rewards[ad_step]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 建置A2C的深度神經網路\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self, n_in, n_mid, n_out):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(n_in, n_mid)\n",
    "        self.fc2 = nn.Linear(n_mid, n_mid)\n",
    "        self.actor = nn.Linear(n_mid, n_out)  # 需要決定動作，所以輸出值為動作的種類數\n",
    "        self.critic = nn.Linear(n_mid, 1)  # 由於這部分是狀態價值，所以輸出值只有一個\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''定義神經網路的前向計算'''\n",
    "        h1 = F.relu(self.fc1(x))\n",
    "        h2 = F.relu(self.fc2(h1))\n",
    "        critic_output = self.critic(h2)  # 計算狀態價值\n",
    "        actor_output = self.actor(h2)  # 計算動作\n",
    "\n",
    "        return critic_output, actor_output\n",
    "\n",
    "    def act(self, x):\n",
    "        '''在狀態x底下以機率計算動作'''\n",
    "        value, actor_output = self(x)\n",
    "        # 以dim=1沿著動作的種類方向計算softmax\n",
    "        action_probs = F.softmax(actor_output, dim=1)\n",
    "        action = action_probs.multinomial(num_samples=1)  # 以dim=1沿著動作的種類方向進行機率計算\n",
    "        return action\n",
    "\n",
    "    def get_value(self, x):\n",
    "        '''根據狀態x計算狀態價值'''\n",
    "        value, actor_output = self(x)\n",
    "\n",
    "        return value\n",
    "\n",
    "    def evaluate_actions(self, x, actions):\n",
    "        '''根據狀態x計算狀態價值、動作action的log機率與熵值'''\n",
    "        value, actor_output = self(x)\n",
    "\n",
    "        log_probs = F.log_softmax(actor_output, dim=1)  # 以dim=1沿著動作種類的方向計算\n",
    "        action_log_probs = log_probs.gather(1, actions)  # 計算實際動作的log_probs\n",
    "\n",
    "        probs = F.softmax(actor_output, dim=1)  # 以dim=1沿著動作種類的方向計算\n",
    "        entropy = -(log_probs * probs).sum(-1).mean()\n",
    "\n",
    "        return value, action_log_probs, entropy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定義智能器的大腦類別，所有智能體共用\n",
    "import torch\n",
    "from torch import optim\n",
    "\n",
    "\n",
    "class Brain(object):\n",
    "    def __init__(self, actor_critic):\n",
    "        self.actor_critic = actor_critic  # actor_critic是Net類別的深度神經網路\n",
    "        self.optimizer = optim.Adam(self.actor_critic.parameters(), lr=0.01)\n",
    "\n",
    "    def update(self, rollouts):\n",
    "        '''於Advantage計算的5個step都用過後再更新'''\n",
    "        obs_shape = rollouts.observations.size()[2:]  # torch.Size([4, 84, 84])\n",
    "        num_steps = NUM_ADVANCED_STEP\n",
    "        num_processes = NUM_PROCESSES\n",
    "\n",
    "        values, action_log_probs, entropy = self.actor_critic.evaluate_actions(\n",
    "            rollouts.observations[:-1].view(-1, 4),\n",
    "            rollouts.actions.view(-1, 1))\n",
    "\n",
    "        # 注意：各變數的大小\n",
    "        # rollouts.observations[:-1].view(-1, 4) torch.Size([80, 4])\n",
    "        # rollouts.actions.view(-1, 1) torch.Size([80, 1])\n",
    "        # values torch.Size([80, 1])\n",
    "        # action_log_probs torch.Size([80, 1])\n",
    "        # entropy torch.Size([])\n",
    "\n",
    "        values = values.view(num_steps, num_processes,\n",
    "                             1)  # torch.Size([5, 16, 1])\n",
    "        action_log_probs = action_log_probs.view(num_steps, num_processes, 1)\n",
    "\n",
    "        # advantage（動作價值-狀態價值）的計算\n",
    "        advantages = rollouts.returns[:-1] - values  # torch.Size([5, 16, 1])\n",
    "\n",
    "        # 計算Critic的loss\n",
    "        value_loss = advantages.pow(2).mean()\n",
    "\n",
    "        # 計算Actor的gain、之後乘上負號，轉換成loss\n",
    "        action_gain = (action_log_probs*advantages.detach()).mean()\n",
    "        # 執行detach，將advantages當成變數使用\n",
    "\n",
    "        # 誤差函數的總和\n",
    "        total_loss = (value_loss * value_loss_coef -\n",
    "                      action_gain - entropy * entropy_coef)\n",
    "\n",
    "        # 更新連結參數\n",
    "        self.actor_critic.train()  # 切換成訓練模式\n",
    "        self.optimizer.zero_grad()  # 重設梯度\n",
    "        total_loss.backward()  # 反向傳播演算法\n",
    "        nn.utils.clip_grad_norm_(self.actor_critic.parameters(), max_grad_norm)\n",
    "        #  為了避免連結參數一下子變化太快，梯度的大小最多為0.5\n",
    "\n",
    "        self.optimizer.step()  # 更新連結參數\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 這次不建構Agent類別"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 這是執行環境的類別\n",
    "import copy\n",
    "\n",
    "\n",
    "class Environment:\n",
    "    def run(self):\n",
    "        '''執行主程式'''\n",
    "\n",
    "        # 依照同時執行所需的環境數量建立env\n",
    "        envs = [gym.make(ENV) for i in range(NUM_PROCESSES)]\n",
    "\n",
    "        # 建構所有智能體共用的大腦Brain\n",
    "        n_in = envs[0].observation_space.shape[0]  # 狀態有4個\n",
    "        n_out = envs[0].action_space.n  # 動作有2個\n",
    "        n_mid = 32\n",
    "        actor_critic = Net(n_in, n_mid, n_out)  # 建置深度神經網路\n",
    "        global_brain = Brain(actor_critic)\n",
    "\n",
    "        # 建立儲存專用變數\n",
    "        obs_shape = n_in\n",
    "        current_obs = torch.zeros(\n",
    "            NUM_PROCESSES, obs_shape)  # torch.Size([16, 4])\n",
    "        rollouts = RolloutStorage(\n",
    "            NUM_ADVANCED_STEP, NUM_PROCESSES, obs_shape)  # rollouts的物件\n",
    "        episode_rewards = torch.zeros([NUM_PROCESSES, 1])  # 儲存目前回合的報酬\n",
    "        final_rewards = torch.zeros([NUM_PROCESSES, 1])  # 儲存最終回合的報酬\n",
    "        obs_np = np.zeros([NUM_PROCESSES, obs_shape])  # Numpy陣列\n",
    "        reward_np = np.zeros([NUM_PROCESSES, 1])  # Numpy陣列\n",
    "        done_np = np.zeros([NUM_PROCESSES, 1])  # Numpy陣列\n",
    "        each_step = np.zeros(NUM_PROCESSES)  # 記錄各環境的step數\n",
    "        episode = 0  # 環境0的回合數\n",
    "\n",
    "        # 初始狀態的啟動\n",
    "        obs = [envs[i].reset() for i in range(NUM_PROCESSES)]\n",
    "        obs = np.array(obs)\n",
    "        obs = torch.from_numpy(obs).float()  # torch.Size([16, 4])\n",
    "        current_obs = obs  # 儲存最新的obs\n",
    "\n",
    "        # 於advanced学習專用物件rollouts的第一個狀態儲存目前的狀態\n",
    "        rollouts.observations[0].copy_(current_obs)\n",
    "\n",
    "        # 執行迴圈\n",
    "        for j in range(NUM_EPISODES*NUM_PROCESSES):  # 整體的for迴圈\n",
    "            # 於advanced學習的每個step計算\n",
    "            for step in range(NUM_ADVANCED_STEP):\n",
    "\n",
    "                # 計算動作\n",
    "                with torch.no_grad():\n",
    "                    action = actor_critic.act(rollouts.observations[step])\n",
    "\n",
    "                # (16,1)→(16,)→將tensor轉換成NumPy\n",
    "                actions = action.squeeze(1).numpy()\n",
    "\n",
    "                # 執行1step\n",
    "                for i in range(NUM_PROCESSES):\n",
    "                    obs_np[i], reward_np[i], done_np[i], _ = envs[i].step(\n",
    "                        actions[i])\n",
    "\n",
    "                    # episode的結束評估與設定state_next\n",
    "                    if done_np[i]:  # 步驟數是否超過200或是棒子超過一定的傾斜角度，done就會轉變成true\n",
    "\n",
    "                        # 只在環境0的時候輸出\n",
    "                        if i == 0:\n",
    "                            print('%d Episode: Finished after %d steps' % (\n",
    "                                episode, each_step[i]+1))\n",
    "                            episode += 1\n",
    "\n",
    "                        # 報酬的設定\n",
    "                        if each_step[i] < 195:\n",
    "                            reward_np[i] = -1.0  # 棒子在中途傾倒便賦予報酬-1作為懲罰\n",
    "                        else:\n",
    "                            reward_np[i] = 1.0  # 若程式在棒子仍直立的時候結束，就賦予報酬1\n",
    "\n",
    "                        each_step[i] = 0  # 重設step數\n",
    "                        obs_np[i] = envs[i].reset()  # 重設執行環境\n",
    "\n",
    "                    else:\n",
    "                        reward_np[i] = 0.0  # 平常狀態的報酬為0\n",
    "                        each_step[i] += 1\n",
    "\n",
    "                # 將報酬轉換成tensor、加入每回合的總報酬\n",
    "                reward = torch.from_numpy(reward_np).float()\n",
    "                episode_rewards += reward\n",
    "\n",
    "                # 各執行環境的狀態不同，若為done就將mask設定為0、若仍在執行，將mask設定為1\n",
    "                masks = torch.FloatTensor(\n",
    "                    [[0.0] if done_ else [1.0] for done_ in done_np])\n",
    "\n",
    "                # 更新最後回合的總報酬\n",
    "                final_rewards *= masks  # 若仍在執行就乘以1，若已經結束執行就乘以0，予以重設\n",
    "                # 若還在執行就加0，若已經是done就加入episode_rewards\n",
    "                final_rewards += (1 - masks) * episode_rewards\n",
    "\n",
    "                # 更新每回合的總報酬\n",
    "                episode_rewards *= masks  # 由於還在執行時的mask為1，所以不會有所改變、若已結束就乘以0\n",
    "\n",
    "                # 若已是done就將目前的狀態全部設定為0\n",
    "                current_obs *= masks\n",
    "\n",
    "                # 更新current_obs\n",
    "                obs = torch.from_numpy(obs_np).float()  # torch.Size([16, 4])\n",
    "                current_obs = obs  # 儲存最新的obs\n",
    "\n",
    "                # 將目前step的transition插入記憶體物件\n",
    "                rollouts.insert(current_obs, action.data, reward, masks)\n",
    "\n",
    "            # advanced的for loop結束\n",
    "\n",
    "            # 從advanced的最後step的狀態計算預設的狀態價值\n",
    "\n",
    "            with torch.no_grad():\n",
    "                next_value = actor_critic.get_value(\n",
    "                    rollouts.observations[-1]).detach()\n",
    "                # rollouts.observations的大小是torch.Size([6, 16, 4])\n",
    "\n",
    "            # 計算所有step的折扣報酬總和，更新rollouts的變數returns\n",
    "            rollouts.compute_returns(next_value)\n",
    "\n",
    "            # 更新神經網路與rollout\n",
    "            global_brain.update(rollouts)\n",
    "            rollouts.after_update()\n",
    "\n",
    "            # 若所有NUM_PROCESSES都為200step，代表學習成功\n",
    "            if final_rewards.sum().numpy() >= NUM_PROCESSES:\n",
    "                print('連續成功')\n",
    "                break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "0 Episode: Finished after 11 steps\n",
      "1 Episode: Finished after 11 steps\n",
      "2 Episode: Finished after 12 steps\n",
      "3 Episode: Finished after 41 steps\n",
      "4 Episode: Finished after 23 steps\n",
      "5 Episode: Finished after 15 steps\n",
      "6 Episode: Finished after 11 steps\n",
      "7 Episode: Finished after 10 steps\n",
      "8 Episode: Finished after 16 steps\n",
      "9 Episode: Finished after 15 steps\n",
      "10 Episode: Finished after 24 steps\n",
      "11 Episode: Finished after 30 steps\n",
      "12 Episode: Finished after 23 steps\n",
      "13 Episode: Finished after 32 steps\n",
      "14 Episode: Finished after 27 steps\n",
      "15 Episode: Finished after 66 steps\n",
      "16 Episode: Finished after 29 steps\n",
      "17 Episode: Finished after 16 steps\n",
      "18 Episode: Finished after 20 steps\n",
      "19 Episode: Finished after 17 steps\n",
      "20 Episode: Finished after 28 steps\n",
      "21 Episode: Finished after 19 steps\n",
      "22 Episode: Finished after 98 steps\n",
      "23 Episode: Finished after 58 steps\n",
      "24 Episode: Finished after 46 steps\n",
      "25 Episode: Finished after 132 steps\n",
      "26 Episode: Finished after 99 steps\n",
      "27 Episode: Finished after 200 steps\n",
      "28 Episode: Finished after 16 steps\n",
      "29 Episode: Finished after 68 steps\n",
      "30 Episode: Finished after 17 steps\n",
      "31 Episode: Finished after 200 steps\n",
      "32 Episode: Finished after 200 steps\n",
      "連続成功\n"
     ]
    }
   ],
   "source": [
    "# main學習\n",
    "cartpole_env = Environment()\n",
    "cartpole_env.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
