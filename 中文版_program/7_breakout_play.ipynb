{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7章　播放打磚塊遊戲Breakout的程式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import套件\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import gym\n",
    "from gym import spaces\n",
    "from gym.spaces.box import Box\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import套件\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "# 宣告動畫的繪製函數\n",
    "# 参考URL http://nbviewer.jupyter.org/github/patrickmineault\n",
    "# /xcorr-notebooks/blob/master/Render%20OpenAI%20gym%20as%20GIF.ipynb\n",
    "from JSAnimation.IPython_display import display_animation\n",
    "from matplotlib import animation\n",
    "from IPython.display import display\n",
    "\n",
    "def display_frames_as_gif(frames):\n",
    "    \"\"\"\n",
    "    Displays a list of frames as a gif, with controls\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(frames[0].shape[1]/72.0*1, frames[0].shape[0]/72.0*1),\n",
    "               dpi=72)\n",
    "    patch = plt.imshow(frames[0])\n",
    "    plt.axis('off')\n",
    " \n",
    "    def animate(i):\n",
    "        patch.set_data(frames[i])\n",
    " \n",
    "    anim = animation.FuncAnimation(plt.gcf(), animate, frames=len(frames),\n",
    "                                   interval=20)\n",
    " \n",
    "    anim.save('breakout.mp4')  # 命令與儲存動畫檔案\n",
    "    display(display_animation(anim, default_mode='loop'))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 設定執行環境\n",
    "# 参考：https://github.com/openai/baselines/blob/master/baselines/common/atari_wrappers.py\n",
    "\n",
    "import cv2\n",
    "cv2.ocl.setUseOpenCL(False)\n",
    "\n",
    "\n",
    "class NoopResetEnv(gym.Wrapper):\n",
    "    def __init__(self, env, noop_max=30):\n",
    "        '''這部分是作業1的No-Operation。要在遊戲重設之後的幾個步驟內，不進行任何操作、\n",
    "        讓遊戲正常初始化，避免在特定的初始狀態學習'''\n",
    "\n",
    "        gym.Wrapper.__init__(self, env)\n",
    "        self.noop_max = noop_max\n",
    "        self.override_num_noops = None\n",
    "        self.noop_action = 0\n",
    "        assert env.unwrapped.get_action_meanings()[0] == 'NOOP'\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        \"\"\" Do no-op action for a number of steps in [1, noop_max].\"\"\"\n",
    "        self.env.reset(**kwargs)\n",
    "        if self.override_num_noops is not None:\n",
    "            noops = self.override_num_noops\n",
    "        else:\n",
    "            noops = self.unwrapped.np_random.randint(\n",
    "                1, self.noop_max + 1)  # pylint: disable=E1101\n",
    "        assert noops > 0\n",
    "        obs = None\n",
    "        for _ in range(noops):\n",
    "            obs, _, done, _ = self.env.step(self.noop_action)\n",
    "            if done:\n",
    "                obs = self.env.reset(**kwargs)\n",
    "        return obs\n",
    "\n",
    "    def step(self, ac):\n",
    "        return self.env.step(ac)\n",
    "\n",
    "\n",
    "class EpisodicLifeEnv(gym.Wrapper):\n",
    "    def __init__(self, env):\n",
    "        '''這是作業2的Episodic Life。掉一顆球之後重設，並從失敗的狀態開始下次的學習'''\n",
    "        gym.Wrapper.__init__(self, env)\n",
    "        self.lives = 0\n",
    "        self.was_real_done = True\n",
    "\n",
    "    def step(self, action):\n",
    "        obs, reward, done, info = self.env.step(action)\n",
    "        self.was_real_done = done\n",
    "        # check current lives, make loss of life terminal,\n",
    "        # then update lives to handle bonus lives\n",
    "        lives = self.env.unwrapped.ale.lives()\n",
    "        if lives < self.lives and lives > 0:\n",
    "            # for Qbert sometimes we stay in lives == 0 condtion for a few frames\n",
    "            # so its important to keep lives > 0, so that we only reset once\n",
    "            # the environment advertises done.\n",
    "            done = True\n",
    "        self.lives = lives\n",
    "        return obs, reward, done, info\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "         '''漏接5顆球之後，徹底重設'''\n",
    "        if self.was_real_done:\n",
    "            obs = self.env.reset(**kwargs)\n",
    "        else:\n",
    "            # no-op step to advance from terminal/lost life state\n",
    "            obs, _, _, _ = self.env.step(0)\n",
    "        self.lives = self.env.unwrapped.ale.lives()\n",
    "        return obs\n",
    "\n",
    "\n",
    "class MaxAndSkipEnv(gym.Wrapper):\n",
    "    def __init__(self, env, skip=4):\n",
    "        '''這是作業3的Max and Skip。連續4格影格執行相同的動作、將最後4格影格的影像轉換成obs'''\n",
    "        gym.Wrapper.__init__(self, env)\n",
    "        # most recent raw observations (for max pooling across time steps)\n",
    "        self._obs_buffer = np.zeros(\n",
    "            (2,)+env.observation_space.shape, dtype=np.uint8)\n",
    "        self._skip = skip\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"Repeat action, sum reward, and max over last observations.\"\"\"\n",
    "        total_reward = 0.0\n",
    "        done = None\n",
    "        for i in range(self._skip):\n",
    "            obs, reward, done, info = self.env.step(action)\n",
    "            if i == self._skip - 2:\n",
    "                self._obs_buffer[0] = obs\n",
    "            if i == self._skip - 1:\n",
    "                self._obs_buffer[1] = obs\n",
    "            total_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "        # Note that the observation on the done=True frame\n",
    "        # doesn't matter\n",
    "        max_frame = self._obs_buffer.max(axis=0)\n",
    "\n",
    "        return max_frame, total_reward, done, info\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        return self.env.reset(**kwargs)\n",
    "\n",
    "\n",
    "class WarpFrame(gym.ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        '''這是作業4的Warp frame。將影像轉換成Nature的DQN論文的84x84黑白影像'''\n",
    "        gym.ObservationWrapper.__init__(self, env)\n",
    "        self.width = 84\n",
    "        self.height = 84\n",
    "        self.observation_space = spaces.Box(low=0, high=255,\n",
    "                                            shape=(self.height, self.width, 1), dtype=np.uint8)\n",
    "\n",
    "    def observation(self, frame):\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)\n",
    "        frame = cv2.resize(frame, (self.width, self.height),\n",
    "                           interpolation=cv2.INTER_AREA)\n",
    "        return frame[:, :, None]\n",
    "\n",
    "\n",
    "class WrapPyTorch(gym.ObservationWrapper):\n",
    "    def __init__(self, env=None):\n",
    "        '''依照PyTorch的小批次索引順序變更的包裹器'''\n",
    "        super(WrapPyTorch, self).__init__(env)\n",
    "        obs_shape = self.observation_space.shape\n",
    "        self.observation_space = Box(\n",
    "            self.observation_space.low[0, 0, 0],\n",
    "            self.observation_space.high[0, 0, 0],\n",
    "            [obs_shape[2], obs_shape[1], obs_shape[0]],\n",
    "            dtype=self.observation_space.dtype)\n",
    "\n",
    "    def observation(self, observation):\n",
    "        return observation.transpose(2, 0, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 播放專用的執行環境\n",
    "\n",
    "\n",
    "class EpisodicLifeEnvPlay(gym.Wrapper):\n",
    "    def __init__(self, env):\n",
    "        '''這是作業2的Episodic Life。掉一顆球之後重設，並從失敗的狀態開始下次的學習\n",
    "        這裡是用於播放學習結果的程式，所以在掉一顆球之後重設遊戲，也在此時重設磚塊的狀態'''\n",
    "\n",
    "        gym.Wrapper.__init__(self, env)\n",
    "\n",
    "    def step(self, action):\n",
    "        obs, reward, done, info = self.env.step(action)\n",
    "        # 生命數（球數）一開始有5個，但只要減少一個就結束程式\n",
    "        if self.env.unwrapped.ale.lives() < 5:\n",
    "            done = True\n",
    "\n",
    "        return obs, reward, done, info\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        '''即使只漏接一次也徹底重設'''\n",
    "\n",
    "        obs = self.env.reset(**kwargs)\n",
    "\n",
    "        return obs\n",
    "\n",
    "\n",
    "class MaxAndSkipEnvPlay(gym.Wrapper):\n",
    "    def __init__(self, env, skip=4):\n",
    "        '''這是作業3的Max and Skip。連續4格影格執行相同的動作、將最後的4格影格的影像轉換成obs'''\n",
    "        gym.Wrapper.__init__(self, env)\n",
    "        # most recent raw observations (for max pooling across time steps)\n",
    "        self._obs_buffer = np.zeros(\n",
    "            (2,)+env.observation_space.shape, dtype=np.uint8)\n",
    "        self._skip = skip\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"Repeat action, sum reward, and max over last observations.\"\"\"\n",
    "        total_reward = 0.0\n",
    "        done = None\n",
    "        for i in range(self._skip):\n",
    "            obs, reward, done, info = self.env.step(action)\n",
    "            if i == self._skip - 2:\n",
    "                self._obs_buffer[0] = obs\n",
    "            if i == self._skip - 1:\n",
    "                self._obs_buffer[1] = obs\n",
    "            total_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        return obs, total_reward, done, info\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        return self.env.reset(**kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定義產生執行環境的函數\n",
    "\n",
    "# 多工執行環境\n",
    "from baselines.common.vec_env.subproc_vec_env import SubprocVecEnv\n",
    "\n",
    "\n",
    "def make_env(env_id, seed, rank):\n",
    "    def _thunk():\n",
    "        '''_thunk()是執行多工環境SubprocVecEnv所需的函數'''\n",
    "\n",
    "        env = gym.make(env_id)\n",
    "        #env = NoopResetEnv(env, noop_max=30)\n",
    "        env = MaxAndSkipEnv(env, skip=4)\n",
    "        env.seed(seed + rank)  # 設定亂數種子\n",
    "        #env = EpisodicLifeEnv(env)\n",
    "        env = EpisodicLifeEnvPlay(env)\n",
    "        env = WarpFrame(env)\n",
    "        env = WrapPyTorch(env)\n",
    "\n",
    "        return env\n",
    "\n",
    "    return _thunk\n",
    "\n",
    "\n",
    "def make_env_play(env_id, seed, rank):\n",
    "    '''播放專用的執行環境'''\n",
    "    env = gym.make(env_id)\n",
    "    #env = NoopResetEnv(env, noop_max=30)\n",
    "    #env = MaxAndSkipEnv(env, skip=4)\n",
    "    env = MaxAndSkipEnvPlay(env, skip=4)\n",
    "    env.seed(seed + rank)  # 設定亂數種子\n",
    "    env = EpisodicLifeEnvPlay(env)\n",
    "    #env = WarpFrame(env)\n",
    "    #env = WrapPyTorch(env)\n",
    "\n",
    "    return env\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 設定常數\n",
    "\n",
    "ENV_NAME = 'BreakoutNoFrameskip-v4' \n",
    "# 使用的不是Breakout-v0，而是BreakoutNoFrameskip-v4\n",
    "# v0會隨機跳過2-4影格，這次使用的是不會隨機跳過影格的版本\n",
    "# 参考URL https://becominghuman.ai/lets-build-an-atari-ai-part-1-dqn-df57e8ff3b26\n",
    "# https://github.com/openai/gym/blob/5cb12296274020db9bb6378ce54276b31e7002da/gym/envs/__init__.py#L371\n",
    "    \n",
    "NUM_SKIP_FRAME = 4 # skip的frame數\n",
    "NUM_STACK_FRAME = 4  # 連續儲存為狀態的frame數\n",
    "NOOP_MAX = 30  #  reset之際，不執行任何作業的影格（No-operation）的前後影格數的亂數上限\n",
    "NUM_PROCESSES = 16 #  多工執行的程序數\n",
    "NUM_ADVANCED_STEP = 5  # 設定執行幾步就要加總一次報酬\n",
    "GAMMA = 0.99  # 時間折扣率\n",
    "\n",
    "TOTAL_FRAMES=10e6  #  用於學習的總影格數\n",
    "NUM_UPDATES = int(TOTAL_FRAMES / NUM_ADVANCED_STEP / NUM_PROCESSES)  # 神經網路的總更新次數\n",
    "# NUM_UPDATESは125,000となる\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 計算A2C損失函數的常數\n",
    "value_loss_coef = 0.5\n",
    "entropy_coef = 0.01\n",
    "max_grad_norm = 0.5\n",
    "\n",
    "# 設定學習手法RMSprop\n",
    "lr = 7e-4\n",
    "eps = 1e-5\n",
    "alpha = 0.99\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "# 使用GPU的設定\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "print(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定義記憶體物件\n",
    "\n",
    "\n",
    "class RolloutStorage(object):\n",
    "    '''學習Advantage所需的記憶體類別'''\n",
    "\n",
    "    def __init__(self, num_steps, num_processes, obs_shape):\n",
    "\n",
    "        self.observations = torch.zeros(\n",
    "            num_steps + 1, num_processes, *obs_shape).to(device)\n",
    "        # 使用*取出()的內容\n",
    "        # obs_shape→(4,84,84)\n",
    "        # *obs_shape→ 4 84 84\n",
    "\n",
    "        self.masks = torch.ones(num_steps + 1, num_processes, 1).to(device)\n",
    "        self.rewards = torch.zeros(num_steps, num_processes, 1).to(device)\n",
    "        self.actions = torch.zeros(\n",
    "            num_steps, num_processes, 1).long().to(device)\n",
    "\n",
    "        # 儲存折扣報酬總和\n",
    "        self.returns = torch.zeros(num_steps + 1, num_processes, 1).to(device)\n",
    "        self.index = 0  # 要insert的索引\n",
    "\n",
    "    def insert(self, current_obs, action, reward, mask):\n",
    "        '''在下一個index存入transition'''\n",
    "        self.observations[self.index + 1].copy_(current_obs)\n",
    "        self.masks[self.index + 1].copy_(mask)\n",
    "        self.rewards[self.index].copy_(reward)\n",
    "        self.actions[self.index].copy_(action)\n",
    "\n",
    "        self.index = (self.index + 1) % NUM_ADVANCED_STEP  # インデックスの更新\n",
    "\n",
    "    def after_update(self):\n",
    "        '''假設執行Advantage的step數歸0，將最新的學習內容存入index0'''\n",
    "        self.observations[0].copy_(self.observations[-1])\n",
    "        self.masks[0].copy_(self.masks[-1])\n",
    "\n",
    "    def compute_returns(self, next_value):\n",
    "        '''加總Advantage每一步的折扣報酬'''\n",
    "\n",
    "        # 注意：從第5步之後開始逆向計算\n",
    "        # 注意：第5步為Advantage1。第4步為Advantage2。・・・\n",
    "        self.returns[-1] = next_value\n",
    "        for ad_step in reversed(range(self.rewards.size(0))):\n",
    "            self.returns[ad_step] = self.returns[ad_step + 1] * \\\n",
    "                GAMMA * self.masks[ad_step + 1] + self.rewards[ad_step]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 建置A2C的深度神經網路\n",
    "\n",
    "\n",
    "def init(module, gain):\n",
    "    '''定義初始化每層連結參數的函數'''\n",
    "    nn.init.orthogonal_(module.weight.data, gain=gain)\n",
    "    nn.init.constant_(module.bias.data, 0)\n",
    "    return module\n",
    "\n",
    "\n",
    "class Flatten(nn.Module):\n",
    "    '''定義將卷積層的輸出影像轉換成一維度的層'''\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x.view(x.size(0), -1)\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, n_out):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        # 連結參數的初始化函數\n",
    "        def init_(module): return init(\n",
    "            module, gain=nn.init.calculate_gain('relu'))\n",
    "\n",
    "        # 卷積層的定義\n",
    "        self.conv = nn.Sequential(\n",
    "            # 影像大小的變化84*84→20*20\n",
    "            init_(nn.Conv2d(NUM_STACK_FRAME, 32, kernel_size=8, stride=4)),\n",
    "            # 堆疊的flame為4影像，所以設定input=NUM_STACK_FRAME=4、並將輸出值設定為32\n",
    "            # size的計算  size = (Input_size - Kernel_size + 2*Padding_size)/ Stride_size + 1\n",
    "\n",
    "            nn.ReLU(),\n",
    "            # 影像大小的變化20*20→9*9\n",
    "            init_(nn.Conv2d(32, 64, kernel_size=4, stride=2)),\n",
    "            nn.ReLU(),\n",
    "            init_(nn.Conv2d(64, 64, kernel_size=3, stride=1)),  # 影像大小的變化9*9→7*7\n",
    "            nn.ReLU(),\n",
    "            Flatten(),  # 將影像格式轉換成一維度\n",
    "            init_(nn.Linear(64 * 7 * 7, 512)),  # 將64張的7×7影像轉換成512維度的output\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # 初始化連結參數的函數\n",
    "        def init_(module): return init(module, gain=1.0)\n",
    "\n",
    "        # Critic的定義\n",
    "        self.critic = init_(nn.Linear(512, 1))  # 因為是狀態價值，所以輸出值只有一個\n",
    "\n",
    "        # 初始化連結參數的函數\n",
    "        def init_(module): return init(module, gain=0.01)\n",
    "\n",
    "        # Actor的定義\n",
    "        self.actor = init_(nn.Linear(512, n_out))  # 行動を決めるので出力は行動の種類数\n",
    "\n",
    "        # 將神經網路切換成訓練模式\n",
    "        self.train()\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''定義神經網路的前向計算'''\n",
    "        input = x / 255.0  # 將影像的像素值從0-255標準化為0-1\n",
    "        conv_output = self.conv(input)  # Convolution層的計算\n",
    "        critic_output = self.critic(conv_output)  # 計算狀態價值\n",
    "        actor_output = self.actor(conv_output)  # 計算行動\n",
    "\n",
    "        return critic_output, actor_output\n",
    "\n",
    "    def act(self, x):\n",
    "        '''在狀態x底下以機率計算動作'''\n",
    "        value, actor_output = self(x)\n",
    "        probs = F.softmax(actor_output, dim=1)    # 以dim=1沿著動作的種類方向進行計算\n",
    "        action = probs.multinomial(num_samples=1)\n",
    "\n",
    "        return action\n",
    "\n",
    "    def get_value(self, x):\n",
    "        '''根據狀態x計算狀態價值'''\n",
    "        value, actor_output = self(x)\n",
    "\n",
    "        return value\n",
    "\n",
    "    def evaluate_actions(self, x, actions):\n",
    "        '''根據狀態x計算狀態價值、動作action的log機率與熵值'''\n",
    "        value, actor_output = self(x)\n",
    "\n",
    "        log_probs = F.log_softmax(actor_output, dim=1)  # 以dim=1沿著動作的種類方向進行計算\n",
    "        action_log_probs = log_probs.gather(1, actions)  # 計算實際動作的log_probs\n",
    "\n",
    "        probs = F.softmax(actor_output, dim=1)  # 以dim=1沿著動作的種類方向進行計算\n",
    "        dist_entropy = -(log_probs * probs).sum(-1).mean()\n",
    "\n",
    "        return value, action_log_probs, dist_entropy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定義智能體的大腦類別，所有智能體共用這個類別\n",
    "\n",
    "\n",
    "class Brain(object):\n",
    "    def __init__(self, actor_critic):\n",
    "\n",
    "        self.actor_critic = actor_critic  # actor_critic是Net類別的深度神經網路\n",
    "\n",
    "        # 載入連結參數的情況\n",
    "        filename = 'weight_end.pth'\n",
    "        #filename = 'weight_112500.pth'\n",
    "        param = torch.load(filename, map_location='cpu')\n",
    "        self.actor_critic.load_state_dict(param)\n",
    "\n",
    "        # 設定更新參數的梯度下降法\n",
    "        self.optimizer = optim.RMSprop(\n",
    "            actor_critic.parameters(), lr=lr, eps=eps, alpha=alpha)\n",
    "\n",
    "    def update(self, rollouts):\n",
    "        '''於Advantage計算的5個step都用過後再更新'''\n",
    "        obs_shape = rollouts.observations.size()[2:]  # torch.Size([4, 84, 84])\n",
    "        num_steps = NUM_ADVANCED_STEP\n",
    "        num_processes = NUM_PROCESSES\n",
    "\n",
    "        values, action_log_probs, dist_entropy = self.actor_critic.evaluate_actions(\n",
    "            rollouts.observations[:-1].view(-1, *obs_shape),\n",
    "            rollouts.actions.view(-1, 1))\n",
    "\n",
    "        # 注意：各變數的大小\n",
    "        # rollouts.observations[:-1].view(-1, *obs_shape) torch.Size([80, 4, 84, 84])\n",
    "        # rollouts.actions.view(-1, 1) torch.Size([80, 1])\n",
    "        # values torch.Size([80, 1])\n",
    "        # action_log_probs torch.Size([80, 1])\n",
    "        # dist_entropy torch.Size([])\n",
    "\n",
    "        values = values.view(num_steps, num_processes,\n",
    "                             1)  # torch.Size([5, 16, 1])\n",
    "        action_log_probs = action_log_probs.view(num_steps, num_processes, 1)\n",
    "\n",
    "        advantages = rollouts.returns[:-1] - values  # torch.Size([5, 16, 1])\n",
    "        value_loss = advantages.pow(2).mean()\n",
    "\n",
    "        action_gain = (advantages.detach() * action_log_probs).mean()\n",
    "        # 執行detach，將advantages當成變數使用\n",
    "\n",
    "        total_loss = (value_loss * value_loss_coef -\n",
    "                      action_gain - dist_entropy * entropy_coef)\n",
    "\n",
    "        self.optimizer.zero_grad()  # 重設梯度\n",
    "        total_loss.backward()  # 反向傳播演算法\n",
    "        nn.utils.clip_grad_norm_(self.actor_critic.parameters(), max_grad_norm)\n",
    "        #  為了避免連結參數一下子變化太快，梯度的大小最多為0.5\n",
    "\n",
    "        self.optimizer.step()  # 更新連結參數\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 這是執行Breakout的環境的類別\n",
    "\n",
    "NUM_PROCESSES = 1\n",
    "\n",
    "\n",
    "class Environment:\n",
    "    def run(self):\n",
    "\n",
    "        # 設定seed\n",
    "        seed_num = 1\n",
    "        torch.manual_seed(seed_num)\n",
    "        if use_cuda:\n",
    "            torch.cuda.manual_seed(seed_num)\n",
    "\n",
    "        # 建置執行環境\n",
    "        torch.set_num_threads(seed_num)\n",
    "        envs = [make_env(ENV_NAME, seed_num, i) for i in range(NUM_PROCESSES)]\n",
    "        envs = SubprocVecEnv(envs)  # 轉換成多工執行環境\n",
    "\n",
    "        # 建立所有智能體共用的大腦Brain\n",
    "        n_out = envs.action_space.n  # 動作的種類共有4種\n",
    "        actor_critic = Net(n_out).to(device)  # 切換成GPU環境\n",
    "        global_brain = Brain(actor_critic)\n",
    "\n",
    "        # 建立儲存專用變數\n",
    "        obs_shape = envs.observation_space.shape  # (1, 84, 84)\n",
    "        obs_shape = (obs_shape[0] * NUM_STACK_FRAME,\n",
    "                     *obs_shape[1:])  # (4, 84, 84)\n",
    "        # torch.Size([16, 4, 84, 84])\n",
    "        current_obs = torch.zeros(NUM_PROCESSES, *obs_shape).to(device)\n",
    "        rollouts = RolloutStorage(\n",
    "            NUM_ADVANCED_STEP, NUM_PROCESSES, obs_shape)  # rollouts的物件\n",
    "        episode_rewards = torch.zeros([NUM_PROCESSES, 1])  # 儲存目前回合的報酬\n",
    "        final_rewards = torch.zeros([NUM_PROCESSES, 1])  # 儲存最終回合的報酬\n",
    "\n",
    "        # 初始狀態的啟動\n",
    "        obs = envs.reset()\n",
    "        obs = torch.from_numpy(obs).float()  # torch.Size([16, 1, 84, 84])\n",
    "        current_obs[:, -1:] = obs  # 於第4格flame儲存最新的obs\n",
    "\n",
    "        # 於advanced学習專用物件rollouts的第一個狀態儲存目前的狀態\n",
    "        rollouts.observations[0].copy_(current_obs)\n",
    "\n",
    "        # 繪製影像專用的環境（播放影像的追加部分）\n",
    "        env_play = make_env_play(ENV_NAME, seed_num, 0)\n",
    "        obs_play = env_play.reset()\n",
    "\n",
    "        # 儲存用於製作動畫的影像（播放影像的追加部分）\n",
    "        frames = []\n",
    "        main_end = False\n",
    "\n",
    "        # 執行迴圈\n",
    "        for j in tqdm(range(NUM_UPDATES)):\n",
    "\n",
    "            # 超過預設的報酬就結束執行（播放影像的追加部分）\n",
    "            if main_end:\n",
    "                break\n",
    "\n",
    "            # 於advanced學習的每個step計算\n",
    "            for step in range(NUM_ADVANCED_STEP):\n",
    "\n",
    "                # 計算動作\n",
    "                with torch.no_grad():\n",
    "                    action = actor_critic.act(rollouts.observations[step])\n",
    "\n",
    "                cpu_actions = action.squeeze(1).cpu().numpy()  # 將tensor轉換成NumPy\n",
    "\n",
    "                # 1step的多工執行，傳回值obs的大小size為(16, 1, 84, 84)\n",
    "                obs, reward, done, info = envs.step(cpu_actions)\n",
    "\n",
    "                # 將報酬轉換成tensor、於每回合的總報酬累計\n",
    "                # 將size為(16,)的轉換成(16, 1)\n",
    "                reward = np.expand_dims(np.stack(reward), 1)\n",
    "                reward = torch.from_numpy(reward).float()\n",
    "                episode_rewards += reward\n",
    "\n",
    "                # 各執行環境的狀態不同，若為done就將mask設定為0、若仍在執行，將mask設定為1\n",
    "                masks = torch.FloatTensor(\n",
    "                    [[0.0] if done_ else [1.0] for done_ in done])\n",
    "\n",
    "                # 更新最後回合的總報酬\n",
    "                final_rewards *= masks  # 若仍在執行就乘以1，若已經結束執行就乘以0，予以重設\n",
    "                # 若還在執行就加0，若已經是done就加入episode_rewards\n",
    "                final_rewards += (1 - masks) * episode_rewards\n",
    "\n",
    "                # 取得影像(播放影像的追加部分）\n",
    "                obs_play, reward_play, _, _ = env_play.step(cpu_actions[0])\n",
    "                frames.append(obs_play)  # 儲存轉換後的影像\n",
    "                if done[0]:  # 第一個多工環境結束時\n",
    "                    print(episode_rewards[0][0].numpy())  # 報酬\n",
    "\n",
    "                    # 報酬超過300就結束程式\n",
    "                    if (episode_rewards[0][0].numpy()) > 300:\n",
    "                        main_end = True\n",
    "                        break\n",
    "                    else:\n",
    "                        obs_view = env_play.reset()\n",
    "                        frames = []  # 重設儲存的影像\n",
    "\n",
    "                # 更新每回合的總報酬\n",
    "                episode_rewards *= masks  # 由於還在執行時的mask為1，所以不會有所改變、若已結束就乘以0\n",
    "\n",
    "                # 將masks切換成GPU\n",
    "                masks = masks.to(device)\n",
    "\n",
    "                # 若已是done就將目前的狀態全部設定為0\n",
    "                # 將mask的サイズ從torch.Size([16, 1])轉換成torch.Size([16, 1, 1 ,1])、再執行乘法\n",
    "                current_obs *= masks.unsqueeze(2).unsqueeze(2)\n",
    "\n",
    "                # 堆疊frame\n",
    "                # torch.Size([16, 1, 84, 84])\n",
    "                obs = torch.from_numpy(obs).float()\n",
    "                current_obs[:, :-1] = current_obs[:, 1:]  # 將第1～3個的obs覆寫至第0～2個obs\n",
    "                current_obs[:, -1:] = obs  # 第4格儲存最新的obs\n",
    "\n",
    "                # 將目前步驟的transition插入記憶體物件\n",
    "                rollouts.insert(current_obs, action.data, reward, masks)\n",
    "\n",
    "            # advanced的for loop結束\n",
    "\n",
    "            # 從advanced的最後step的狀態計算預設的狀態價值\n",
    "            with torch.no_grad():\n",
    "                next_value = actor_critic.get_value(\n",
    "                    rollouts.observations[-1]).detach()\n",
    "\n",
    "            # 計算所有step的折扣報酬總和，更新rollouts的變數returns\n",
    "            rollouts.compute_returns(next_value)\n",
    "\n",
    "            # 更新神經網路與rollout\n",
    "            # global_brain.update(rollouts)\n",
    "            rollouts.after_update()\n",
    "\n",
    "        # 結束執行迴圈\n",
    "        display_frames_as_gif(frames)  # 儲存與播放影片\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 執行\n",
    "breakout_env = Environment()\n",
    "frames = breakout_env.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
